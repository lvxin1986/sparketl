package com.jd.etl

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{StructField, StructType}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Created by root on 17-8-1.
  */
object Mysql2Orc {
  def main(args: Array[String]) {
    val hivetableName = args(0)
    var orcPath= args(1)
    val spark = SparkSession.builder.appName("Mysql2ORc") // optional and will be autogenerated if not specified
      .master("yarn")               // avoid hardcoding the deployment environment
      .enableHiveSupport()              // self-explanatory, isn't it?
      .config("spark.sql.warehouse.dir", "target/spark-warehouse")
      .config("spark.shuffle.service.enabled","true")
    .config("spark.dynamicAllocation.enabled","true")
    .config("spark.executor.cores","5").getOrCreate
    spark.conf.set("spark.sql.shuffle.partitions", 1)
    spark.conf.set("spark.executor.memory", "2g")
    var result = spark.sql("select * from "+hivetableName)
    result.write.orc(orcPath)
  }
}
