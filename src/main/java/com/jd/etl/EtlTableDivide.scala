package com.jd.etl

import java.text.SimpleDateFormat
import java.util.concurrent.Executors
import java.util.{Calendar, Properties}

import com.jd.etl.consts.ETLConst
import com.jd.etl.utils.{EtlUtil, ColumnUtil}
import org.apache.spark.sql.types.{IntegerType, LongType}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.storage.StorageLevel

import scala.collection.mutable
import scala.concurrent._
import scala.concurrent.duration._
import scala.util.{Failure, Success}

/**
  * Created by ninet on 17-8-21.
  */
object EtlTableDivide {
    val hadoopConf = new org.apache.hadoop.conf.Configuration()
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)
    val dateFormat = new SimpleDateFormat(ETLConst.ETL_DATE_FORMAT)
    var persistStatus: mutable.Map[String, Boolean] = collection.mutable.Map[String, Boolean]()

    def main(args: Array[String]) {

        etlDataDivideTable(args(0), args(1), args(2), args(3))

        System.exit(0)
    }
/*
* etlDataDivideTable
* 以分区下分表的形式抽取数据
* */
    def etlDataDivideTable(workSpace: String, filterCol: String, filterVal1: String, filterVal2: String): Unit = {
        val spark = SparkSession.builder.appName("EtlSplitTable_2_Orc_Daily") // optional and will be autogenerated if not specified
            .enableHiveSupport() // self-explanatory, isn't it?
            .getOrCreate

        var config = new EtlConfig(workSpace, spark)
        val tblsArray: Array[String] = spark.sparkContext.textFile(workSpace + ETLConst.ETL_TABLELIST_FILE).collect()
        val schemasArray: Array[String] = spark.sparkContext.textFile(workSpace + ETLConst.ETL_SCHEMALIST_FILE).collect()
        val pool = Executors.newFixedThreadPool(config.threadsNum)
        implicit val xc = ExecutionContext.fromExecutorService(pool)
        val tasks: mutable.MutableList[Future[String]] = mutable.MutableList[Future[String]]()
        val schemaMap: Map[String, String] = EtlUtil.initSchemaMap(schemasArray)

        for (i <- 0 until tblsArray.length) {
            val tbl_schema_pair: Array[String] = tblsArray.apply(i).split("\\s+")
            val tblName = tbl_schema_pair.apply(0)
            val sql: String = EtlUtil.generateSql(schemaMap, tbl_schema_pair, filterCol, filterVal1, filterVal2)
            val status: mutable.MutableList[String] = mutable.MutableList[String]()
            val partition_col_name_type_pair = filterCol.split(":")
            val col_name = partition_col_name_type_pair.apply(0)
            val col_type = partition_col_name_type_pair.apply(1)
            var start_time = filterVal1 + " 00:00:00"
            var end_time = filterVal2 + " 00:00:00"
            var quotation = ""

            try {
                var df = EtlUtil.castInt2BigInt(spark.sql(sql))
                df = df.persist(StorageLevel.MEMORY_AND_DISK_SER)
                println("[INFO] execute the sql : " + sql + " and store in memory and disk!")

                persistStatus += (tblName -> false)
                if (!ETLConst.COL_TYPE_NUMBER.contains(col_type.toUpperCase())) {
                    quotation = "'"
                }

                /*if (!EtlUtil.dirExists(fs, config.tmpPath + tblName.split("\\.").apply(1))) {
                    createTmpTable(spark, tblName.split("\\.").apply(1), config)
                }*/

                while (!start_time.equals(end_time)) {
                    var cal: Calendar = Calendar.getInstance();
                    cal.setTime(dateFormat.parse(end_time))
                    cal.add(Calendar.DATE, -1)
                    var yesterday = dateFormat.format(cal.getTime)
                    val partitionPath = "/" + config.targetPartition + "=" + yesterday.substring(0, 10) + "/"
                    val condition = col_name + ">=" + quotation + yesterday + quotation + " and " + col_name + "<" + quotation + end_time + quotation
                    val totalPath = config.targetPath + config.targetTable + partitionPath + tblName + "/"
                    if (EtlUtil.dirExists(fs, totalPath + "_SUCCESS")) {
                        println("[WARN]" + totalPath + "_SUCCESS exist, skipped it")
                    } else {
                        if (EtlUtil.dirExists(fs, totalPath)) {
                            println("[WARN]" + totalPath + " exist, but " + totalPath + "_SUCCESS not exist,removing " + totalPath + " and reimport")
                            fs.delete(new org.apache.hadoop.fs.Path(totalPath), true)
                        }
                        if (persistStatus.get(tblName).get == false) {
                            println("[INFO] First execute for table " + tblName + "with condition " + condition)
                            status += doEtlPerDate(df, condition, totalPath, config.orcCoalesce)
                            persistStatus.put(tblName, true)
                        } else {
                            println("[INFO] More execute for table " + tblName + "with condition " + condition)
                            val task = doEtlPerTbl(df, condition, totalPath, config.orcCoalesce)
                            task.onComplete {
                                case Success(result) =>
                                    println(s"result = $result")
                                case Failure(e) =>
                                    println(e.getMessage)
                                    status += e.getMessage
                            }
                            tasks += task
                        }
                    }
                    end_time = yesterday
                }

                Await.ready(Future.sequence(tasks), Duration(30, DAYS))
                println("[INFO] All Jobs has completed and the informations are:")
                for (i <- 0 until status.length) {
                    println("[INFO]" + status(i))
                }
                df.unpersist()
                tasks.clear()
            } catch {
                case e: Throwable => {
                    println("[ERROR] Table: " + tblName + " write the orc file failed")
                    println(e)
                }
            } finally {
                tasks.clear()
            }
        }
        println("[INFO] All Jobs has completed")
        spark.close()
    }

    /*def createTmpTable(spark: SparkSession, tblName: String, config: EtlConfig): Unit = {
        var createTableSql = "create table " + config.tmpDb + "." + tblName + " like " + config.sampleTable
        spark.sql(createTableSql)
        println("[INFO] execute the sql : " + createTableSql + " and create table success!")
    }*/

    def doEtlPerDate(df: DataFrame, condition: String, outPath: String, orcCoalesce: Int): String = {
        var message = "[INFO] filter the dataframe : " + condition + " and write the orc file in " + outPath + " with orcCoalesce: " + orcCoalesce + " for first time Synchronous successfully!"
        println("[INFO] The final schema infomation is:")
        df.printSchema()
        println("[INFO] Start filter and write data to path:" + outPath)
        if (0 == orcCoalesce) {
            if (condition == null || condition.equals("")) {
                df.write.orc(outPath)
            } else {
                df.filter(condition).write.orc(outPath)
            }
        } else {
            if (condition == null || condition.equals("")) {
                df.coalesce(orcCoalesce).write.orc(outPath)
            } else {
                df.filter(condition).coalesce(orcCoalesce).write.orc(outPath)
            }
        }
        message
    }


    def doEtlPerTbl(df: DataFrame, condition: String, outPath: String, orcCoalesce: Int)(implicit xc: ExecutionContext) = Future {
        var message = "[INFO] filter the dataframe : " + condition + " and write the orc file in " + outPath + " with orcCoalesce: " + orcCoalesce + " asynchronous successfully!"
        println("[INFO] The final schema infomation is:")
        df.printSchema()
        println("[INFO] Start filter and write data to path:" + outPath)
        if (0 == orcCoalesce) {
            if (condition == null || condition.equals("")) {
                df.write.orc(outPath)
            } else {
                df.filter(condition).write.orc(outPath)
            }
        } else {
            if (condition == null || condition.equals("")) {
                df.coalesce(orcCoalesce).write.orc(outPath)
            } else {
                df.filter(condition).coalesce(orcCoalesce).write.orc(outPath)
            }
        }
        message
    }
}
