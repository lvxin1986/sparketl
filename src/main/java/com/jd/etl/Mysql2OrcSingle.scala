package com.jd.etl

import java.io.ByteArrayInputStream
import java.lang.Exception
import java.util.Properties

import org.apache.spark.sql.SparkSession
import java.util.concurrent.Executors

import com.jd.etl.consts.ETLConst

import scala.collection.mutable
import scala.concurrent._
import scala.concurrent.duration._
import scala.util.{Failure, Success}

/**
  * Created by root on 17-8-1.
  */
object Mysql2OrcSingle {
    val hadoopConf = new org.apache.hadoop.conf.Configuration()
    val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)

    def main(args: Array[String]) {
        if (args.size == 1) {
            InitData(args(0))
        } else {
            ImportDataDaily(args(0), args(1), args(2), args(3))
        }
    }

    def InitData(confPath: String): Unit = {
        val spark = SparkSession.builder.appName("Mysql_2_Orc_Init_No_Partition").enableHiveSupport().getOrCreate

        var confArray: Array[String] = spark.sparkContext.textFile(confPath).collect()
        var conf: String = confArray.mkString("\n")
        val properties = new Properties
        val inputStream = new ByteArrayInputStream(conf.getBytes)
        properties.load(inputStream)
        val threadsNum = properties.getProperty(ETLConst.ETL_THREADS, ETLConst.ETL_THREADS_DEFAUL_VALUE).toInt
        val tablesPath = properties.getProperty(ETLConst.ETL_TABLES_PATH)
        val orcPath = properties.getProperty(ETLConst.ETL_TAGET_PATH)
        val orcCoalesce = properties.getProperty(ETLConst.ETL_COALESCE, ETLConst.ETL_COALESCE_DEFAULT_VALUE).toInt
        println("[DEBUG] "+ETLConst.ETL_COALESCE+" = "+ orcCoalesce)
        
        val tblsArray: Array[String] = spark.sparkContext.textFile(tablesPath).collect()
        for (i <- 0 until tblsArray.length) {
            var sql = "select * from " + tblsArray(i)
            if (dirExists(orcPath + tblsArray(i) + "/_SUCCESS")) {
                println("[WARN]" + orcPath + tblsArray(i) + "/_SUCCESS exist, skipped it")
            } else {
                if (dirExists(orcPath + tblsArray(i))) {
                    println("[WARN]" + orcPath + tblsArray(i) + " exist, but " + orcPath + tblsArray(i) + "/_SUCCESS not exist,removing " + orcPath + tblsArray(i) + " and reimport")
                    fs.delete(new org.apache.hadoop.fs.Path(orcPath + tblsArray(i)), true)
                }
                doEtlPerTbl(spark, sql, orcPath + tblsArray(i), orcCoalesce)
            }

        }

        spark.close()

    }

    def ImportDataDaily(confPath: String, filterCol: String, filterVal1: String, filterVal2: String): Unit = {
        val spark = SparkSession.builder.appName("Mysql_2_ORc_Import_Daily") // optional and will be autogenerated if not specified
          .enableHiveSupport() // self-explanatory, isn't it?
          .getOrCreate

        val confArray: Array[String] = spark.sparkContext.textFile(confPath).collect()
        val conf: String = confArray.mkString("\n")
        val properties = new Properties
        val inputStream = new ByteArrayInputStream(conf.getBytes)
        properties.load(inputStream)
        val threadsNum = properties.getProperty(ETLConst.ETL_THREADS, ETLConst.ETL_THREADS_DEFAUL_VALUE).toInt
        val tablesPath = properties.getProperty(ETLConst.ETL_TABLES_PATH)
        val orcPath = properties.getProperty(ETLConst.ETL_TAGET_PATH)
        val orcCoalesce = properties.getProperty(ETLConst.ETL_COALESCE, ETLConst.ETL_COALESCE_DEFAULT_VALUE).toInt

        var tblsArray: Array[String] = spark.sparkContext.textFile(tablesPath).collect()
        for (i <- 0 until tblsArray.length) {
            var sql = "select * from " + tblsArray(i)
            var partitionPath = "";
            if (filterCol != null && !filterCol.equals("") && filterVal1 != null && !filterVal1.equals("") && filterVal2 != null && !filterVal2.equals("")) {
                val columns: Array[String] = filterCol.split(",")
                val values1: Array[String] = filterVal1.split(",")
                val values2: Array[String] = filterVal2.split(",")
                if (columns.length == values1.length) {
                    for (j <- 0 until columns.length) {
                        val c_t = columns.apply(j).split(":")
                        val col_type = c_t.apply(1)
                        var quotation = "";
                        if(!ETLConst.COL_TYPE_NUMBER.contains(col_type.toUpperCase())){
                            quotation = "'"
                        }

                        if (j == 0) {
                            sql = sql + " where " + c_t.apply(0) + ">"+quotation + values1.apply(j) + " 00:00:00"+quotation + " and " + columns.apply(j) + "<"+quotation + values2.apply(j) + " 00:00:00"+quotation
                        } else {
                            sql = sql + " and " + columns.apply(j) + ">"+quotation + values1.apply(j) + " 00:00:00"+quotation + " and " + columns.apply(j) + "<"+quotation + values2.apply(j) + " 00:00:00"+quotation
                        }
                        partitionPath = partitionPath + "/" + columns.apply(j) + "=" + values1.apply(j)
                    }
                } else {
                    println("[ERROR] columns length is not equal with values length")
                }
            }
            if (dirExists(orcPath + tblsArray(i) + partitionPath + "/_SUCCESS")) {
                println("[WARN]" + orcPath + tblsArray(i) + partitionPath + "/_SUCCESS exist, skipped it")
            } else {
                if (dirExists(orcPath + tblsArray(i) + partitionPath)) {
                    println("[WARN]" + orcPath + tblsArray(i) + partitionPath + " exist, but " + orcPath + tblsArray(i) + partitionPath + "/_SUCCESS not exist,removing " + orcPath + tblsArray(i) + partitionPath + " and reimport")
                    fs.delete(new org.apache.hadoop.fs.Path(orcPath + tblsArray(i) + partitionPath), true)
                }
                doEtlPerTbl(spark, sql, orcPath + tblsArray(i) + partitionPath, orcCoalesce)
            }

        }
        spark.close()
    }



    def doEtlPerTbl(ss: SparkSession, sql: String, outPath: String, orcCoalesce: Int):Unit = {
        var message = "NULl";
        if(0 == orcCoalesce){
            ss.sql(sql).write.orc(outPath)
        }else {
            ss.sql(sql).coalesce(orcCoalesce).write.orc(outPath)
        }

        message = "[INFO] execute the sql : " + sql + " and write the orc file in " + outPath + " with orcCoalesce = "+ orcCoalesce+" successfully!"
        message

    }

    def dirExists(hdfsDirectory: String): Boolean = {
        val exists = fs.exists(new org.apache.hadoop.fs.Path(hdfsDirectory))
        return exists
    }
}
